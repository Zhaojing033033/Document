RDD是spark中的一个最基本的抽象，代表着一个不可变、可分区、可以并行计算的分布式数据集
RDD有5个特点
	1.一系列分区
	2.会有一个函数作用在每个切片上
	3.RDD和RDD之间存在依赖关系
	4（可选）如果是RDD中装的是KV类型的，那么Shuffle时会有一个分区器。默认是HashPartitioner
	5（可选）如果只从HDFS中读取数据，会感知数据则位置，将Executor启动在数据所在的机器上

Spark的设计就是基于这个抽象的数据集（RDD），你操作RDD这个抽象的数据集，就像操作一个本地集合一样，Spark包底层的细节都隐藏起来的（任务调度、Task执行，任务失败重试等待），开发者使用起来更加方便简洁

操作RDD，其实是对每个分区进行操作，分区会生成Task，Task会调度Executor上执行相关的计算逻辑，进而对数据进操作


